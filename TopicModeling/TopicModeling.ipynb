{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling Using LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description: Topic Modeling is a type of statistical modeling for discovering the abstract topics that occur in a collection of documents.\n",
    "\n",
    "Latent Dirichlet Allocation (LDA) is an example of topic model and is used to classify text in a document to a particular topic. It builds a topic per document model and words per topic model, modeled as Dirichlet distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora, models\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>by</th>\n",
       "      <th>category</th>\n",
       "      <th>comment_likes_count</th>\n",
       "      <th>comments_base</th>\n",
       "      <th>comments_count_fb</th>\n",
       "      <th>comments_replies</th>\n",
       "      <th>comments_retrieved</th>\n",
       "      <th>engagement_fb</th>\n",
       "      <th>likes_count_fb</th>\n",
       "      <th>post_message</th>\n",
       "      <th>post_published</th>\n",
       "      <th>rea_ANGRY</th>\n",
       "      <th>rea_HAHA</th>\n",
       "      <th>rea_LOVE</th>\n",
       "      <th>rea_SAD</th>\n",
       "      <th>rea_THANKFUL</th>\n",
       "      <th>rea_WOW</th>\n",
       "      <th>reactions_count_fb</th>\n",
       "      <th>shares_count_fb</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>post_page_155027942462</td>\n",
       "      <td>App Update</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>11</td>\n",
       "      <td>['paying', 'bill', 'autopay', 'ensures', 'bill...</td>\n",
       "      <td>2019-06-27T14:30:13+0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>photo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>post_page_155027942462</td>\n",
       "      <td>Engagement</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>110</td>\n",
       "      <td>70</td>\n",
       "      <td>['raise', 'hand', 'excited', 'first', 'day', '...</td>\n",
       "      <td>2019-06-21T14:31:06+0000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>76</td>\n",
       "      <td>4</td>\n",
       "      <td>photo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>post_page_155027942462</td>\n",
       "      <td>Engagement</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>5</td>\n",
       "      <td>['couple', 'save', 'together', 'stay', 'foreve...</td>\n",
       "      <td>2019-06-20T15:00:28+0000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>photo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>post_page_155027942462</td>\n",
       "      <td>Engagement</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>8</td>\n",
       "      <td>['case', 'forgotten', 'make', 'saving', 'daily...</td>\n",
       "      <td>2019-06-17T14:30:11+0000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>photo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>post_page_155027942462</td>\n",
       "      <td>Engagement</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>6</td>\n",
       "      <td>['father', 'day', 'approaching', 'got', 'smart...</td>\n",
       "      <td>2019-06-14T14:30:02+0000</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>video</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       by    category  comment_likes_count  comments_base  \\\n",
       "0  post_page_155027942462  App Update                    0              0   \n",
       "1  post_page_155027942462  Engagement                    0              0   \n",
       "2  post_page_155027942462  Engagement                    0              0   \n",
       "3  post_page_155027942462  Engagement                    0              0   \n",
       "4  post_page_155027942462  Engagement                    0              0   \n",
       "\n",
       "   comments_count_fb  comments_replies  comments_retrieved  engagement_fb  \\\n",
       "0                 11                 0                   0             22   \n",
       "1                 30                 0                   0            110   \n",
       "2                 11                 0                   0             17   \n",
       "3                 10                 0                   0             19   \n",
       "4                  8                 0                   0             17   \n",
       "\n",
       "   likes_count_fb                                       post_message  \\\n",
       "0              11  ['paying', 'bill', 'autopay', 'ensures', 'bill...   \n",
       "1              70  ['raise', 'hand', 'excited', 'first', 'day', '...   \n",
       "2               5  ['couple', 'save', 'together', 'stay', 'foreve...   \n",
       "3               8  ['case', 'forgotten', 'make', 'saving', 'daily...   \n",
       "4               6  ['father', 'day', 'approaching', 'got', 'smart...   \n",
       "\n",
       "             post_published  rea_ANGRY  rea_HAHA  rea_LOVE  rea_SAD  \\\n",
       "0  2019-06-27T14:30:13+0000          0         0         0        0   \n",
       "1  2019-06-21T14:31:06+0000          1         0         4        0   \n",
       "2  2019-06-20T15:00:28+0000          1         0         0        0   \n",
       "3  2019-06-17T14:30:11+0000          1         0         0        0   \n",
       "4  2019-06-14T14:30:02+0000          3         0         0        0   \n",
       "\n",
       "   rea_THANKFUL  rea_WOW  reactions_count_fb  shares_count_fb   type  \n",
       "0             0        0                  11                0  photo  \n",
       "1             0        1                  76                4  photo  \n",
       "2             0        0                   6                0  photo  \n",
       "3             0        0                   9                0  photo  \n",
       "4             0        0                   9                0  video  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv('fullstatCleaned_withLabels.tsv', sep='\\t')\n",
    "# Inspect df\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selct necessary columns\n",
    "processed_words = df['post_message']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words on the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tokenized string for BoW input\n",
    "processed_words = [words.split() for words in processed_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary from ‘processed_words’ containing the number of times a word appears in the training set\n",
    "dictionary = gensim.corpora.Dictionary(processed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out tokens\n",
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary reporting how many words and how many times those words appear\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 48 (\"'payday',\") appears 1 time.\n",
      "Word 60 (\"'see',\") appears 1 time.\n",
      "Word 92 (\"'week',\") appears 1 time.\n",
      "Word 102 (\"'bonus',\") appears 1 time.\n",
      "Word 148 (\"'winner']\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "# Preview Bag of Words for a sample preprocessed document\n",
    "bow_doc_500 = bow_corpus[500]\n",
    "\n",
    "for i in range(len(bow_doc_500)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_500[i][0], \n",
    "                                                     dictionary[bow_doc_500[i][0]], \n",
    "                                                     bow_doc_500[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tf-idf model object\n",
    "tfidf = models.TfidfModel(bow_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply transformation to the entire corpus\n",
    "corpus_tfidf = tfidf[bow_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.8509381336929653), (1, 0.40105490505841873), (2, 0.33920385575007195)]\n"
     ]
    }
   ],
   "source": [
    "# Preview TF-IDF scores for the first document\n",
    "for doc in corpus_tfidf:\n",
    "    pprint(doc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running LDA using Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the lda model using gensim.models.LdaMulticore and bow_corpus\n",
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=10, id2word=dictionary, passes=2, workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.083*\"'get',\" + 0.063*\"'pay',\" + 0.050*\"'day',\" + 0.040*\"'period',\" + 0.040*\"'deposit',\" + 0.024*\"'like',\" + 0.023*\"'direct',\" + 0.023*\"'employer',\" + 0.022*\"'may',\" + 0.022*\"'notice',\"\n",
      "\n",
      "Topic: 1 \n",
      "Words: 0.065*\"'u',\" + 0.046*\"'tell',\" + 0.043*\"'dot',\" + 0.043*\"'green',\" + 0.038*\"'comment']\" + 0.030*\"'get',\" + 0.025*\"'back',\" + 0.025*\"'see',\" + 0.022*\"'weekend',\" + 0.019*\"'tax',\"\n",
      "\n",
      "Topic: 2 \n",
      "Words: 0.085*\"'see',\" + 0.077*\"'week',\" + 0.069*\"'payday',\" + 0.063*\"'bonus',\" + 0.053*\"'someone',\" + 0.035*\"'check',\" + 0.034*\"'person']\" + 0.032*\"'ontheblog']\" + 0.028*\"'winner']\" + 0.027*\"['learn',\"\n",
      "\n",
      "Topic: 3 \n",
      "Words: 0.045*\"'direct',\" + 0.042*\"'get',\" + 0.042*\"'asap',\" + 0.040*\"'deposit',\" + 0.036*\"'paid',\" + 0.032*\"'payment',\" + 0.031*\"'finance',\" + 0.031*\"'win',\" + 0.030*\"'day',\" + 0.030*\"'play',\"\n",
      "\n",
      "Topic: 4 \n",
      "Words: 0.054*\"'u',\" + 0.047*\"'tip',\" + 0.047*\"'like',\" + 0.038*\"'comment',\" + 0.034*\"'see',\" + 0.030*\"'sweepstakes',\" + 0.030*\"'financial',\" + 0.027*\"'page',\" + 0.025*\"'tell',\" + 0.022*\"'win',\"\n",
      "\n",
      "Topic: 5 \n",
      "Words: 0.086*\"'back',\" + 0.083*\"'cash',\" + 0.054*\"'green',\" + 0.053*\"'dot',\" + 0.050*\"'card',\" + 0.044*\"'visa',\" + 0.042*\"'debit',\" + 0.042*\"'fee',\" + 0.038*\"'annually',\" + 0.026*\"'get',\"\n",
      "\n",
      "Topic: 6 \n",
      "Words: 0.080*\"'card',\" + 0.078*\"'dot',\" + 0.059*\"'green',\" + 0.032*\"'mobile',\" + 0.027*\"'prepaid',\" + 0.027*\"'tax',\" + 0.026*\"'app',\" + 0.026*\"'account',\" + 0.026*\"'credit',\" + 0.025*\"'balance',\"\n",
      "\n",
      "Topic: 7 \n",
      "Words: 0.058*\"'dot',\" + 0.056*\"'green',\" + 0.054*\"'sweepstakes',\" + 0.042*\"'money',\" + 0.031*\"'advice',\" + 0.025*\"'see',\" + 0.023*\"'u',\" + 0.023*\"'love',\" + 0.022*\"'entrant']\" + 0.021*\"'comment',\"\n",
      "\n",
      "Topic: 8 \n",
      "Words: 0.046*\"'card',\" + 0.043*\"'win',\" + 0.030*\"'fee',\" + 0.028*\"'cash',\" + 0.026*\"'chance',\" + 0.024*\"'dot',\" + 0.023*\"'back',\" + 0.023*\"'green',\" + 0.023*\"'apply',\" + 0.022*\"'tax',\"\n",
      "\n",
      "Topic: 9 \n",
      "Words: 0.057*\"'card',\" + 0.044*\"'money',\" + 0.036*\"'learn',\" + 0.030*\"'save',\" + 0.029*\"'limit',\" + 0.027*\"'cash',\" + 0.026*\"'moneypak',\" + 0.025*\"'green',\" + 0.025*\"'dot',\" + 0.024*\"'share',\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For each topic, explore the words occuring in that topic and its relative weight.\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}\\n'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running LDA using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the lda model using gensim.models.LdaMulticore and corpus_tfidf\n",
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=10, id2word=dictionary, passes=2, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Word: 0.097*\"'financial',\" + 0.037*\"'goal',\" + 0.030*\"'new',\" + 0.029*\"'comment']\" + 0.028*\"'best',\" + 0.027*\"'someone',\" + 0.027*\"'credit',\" + 0.025*\"['learn',\" + 0.024*\"'one',\" + 0.024*\"'learn',\"\n",
      "\n",
      "Topic: 1 \n",
      "Word: 0.056*\"['green',\" + 0.034*\"'tax',\" + 0.029*\"'comment',\" + 0.027*\"'dot',\" + 0.027*\"'money',\" + 0.022*\"'tip']\" + 0.021*\"'prize',\" + 0.021*\"'finance',\" + 0.021*\"'smart',\" + 0.021*\"'giving',\"\n",
      "\n",
      "Topic: 2 \n",
      "Word: 0.047*\"'u',\" + 0.046*\"'tell',\" + 0.038*\"'day',\" + 0.035*\"'credit',\" + 0.027*\"'moneypak',\" + 0.026*\"'card',\" + 0.023*\"'check',\" + 0.023*\"'get',\" + 0.020*\"'comment',\" + 0.018*\"'using',\"\n",
      "\n",
      "Topic: 3 \n",
      "Word: 0.060*\"'money',\" + 0.051*\"'read',\" + 0.049*\"'save',\" + 0.043*\"'ontheblog']\" + 0.039*\"'finance',\" + 0.031*\"'financial',\" + 0.025*\"['learn',\" + 0.025*\"'way',\" + 0.025*\"'read']\" + 0.021*\"'u',\"\n",
      "\n",
      "Topic: 4 \n",
      "Word: 0.035*\"'tip',\" + 0.035*\"'budget',\" + 0.026*\"'back',\" + 0.025*\"'like',\" + 0.023*\"'u',\" + 0.023*\"'saving',\" + 0.023*\"'page',\" + 0.022*\"'cash',\" + 0.022*\"'sweepstakes',\" + 0.022*\"'let',\"\n",
      "\n",
      "Topic: 5 \n",
      "Word: 0.063*\"'payment',\" + 0.060*\"'ssi',\" + 0.034*\"'stay',\" + 0.032*\"'see',\" + 0.031*\"'tip',\" + 0.029*\"'benefit',\" + 0.029*\"'time',\" + 0.028*\"'person']\" + 0.024*\"'week',\" + 0.023*\"'someone',\"\n",
      "\n",
      "Topic: 6 \n",
      "Word: 0.049*\"'back',\" + 0.042*\"'cash',\" + 0.034*\"'love',\" + 0.028*\"'visa',\" + 0.026*\"'green',\" + 0.026*\"'dot',\" + 0.026*\"'card',\" + 0.025*\"'debit',\" + 0.025*\"'favorite',\" + 0.023*\"'bonus',\"\n",
      "\n",
      "Topic: 7 \n",
      "Word: 0.040*\"'share',\" + 0.038*\"'mondaymotivation']\" + 0.029*\"'day',\" + 0.028*\"'plan',\" + 0.025*\"'extra',\" + 0.024*\"'customer',\" + 0.024*\"'pay',\" + 0.022*\"'entrant',\" + 0.022*\"'reflects',\" + 0.022*\"'statement',\"\n",
      "\n",
      "Topic: 8 \n",
      "Word: 0.046*\"['happy',\" + 0.036*\"'family',\" + 0.033*\"'make',\" + 0.031*\"'balance',\" + 0.031*\"'financialliteracymonth']\" + 0.024*\"'green',\" + 0.023*\"'card',\" + 0.021*\"'like',\" + 0.020*\"'account',\" + 0.019*\"'fee',\"\n",
      "\n",
      "Topic: 9 \n",
      "Word: 0.035*\"'way',\" + 0.033*\"'advice',\" + 0.033*\"'green',\" + 0.030*\"'dot',\" + 0.029*\"'app',\" + 0.025*\"'mobile',\" + 0.023*\"'win',\" + 0.022*\"'learn']\" + 0.021*\"'sweepstakes',\" + 0.020*\"'winner']\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For each topic, explore the words occuring in that topic and its relative weight.\n",
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} \\nWord: {}\\n'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification of the Topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance evaluation by classifying sample document using LDA Bag of Words model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"['someone',\",\n",
       " \"'cloud',\",\n",
       " \"'nine',\",\n",
       " \"'see',\",\n",
       " \"'week',\",\n",
       " \"'payday',\",\n",
       " \"'bonus',\",\n",
       " \"'winner']\"]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_words[500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.849994421005249\t \n",
      "Topic: 0.085*\"'see',\" + 0.077*\"'week',\" + 0.069*\"'payday',\" + 0.063*\"'bonus',\" + 0.053*\"'someone',\" + 0.035*\"'check',\" + 0.034*\"'person']\" + 0.032*\"'ontheblog']\" + 0.028*\"'winner']\" + 0.027*\"['learn',\"\n",
      "\n",
      "Score: 0.01666816882789135\t \n",
      "Topic: 0.080*\"'card',\" + 0.078*\"'dot',\" + 0.059*\"'green',\" + 0.032*\"'mobile',\" + 0.027*\"'prepaid',\" + 0.027*\"'tax',\" + 0.026*\"'app',\" + 0.026*\"'account',\" + 0.026*\"'credit',\" + 0.025*\"'balance',\"\n",
      "\n",
      "Score: 0.016667695716023445\t \n",
      "Topic: 0.058*\"'dot',\" + 0.056*\"'green',\" + 0.054*\"'sweepstakes',\" + 0.042*\"'money',\" + 0.031*\"'advice',\" + 0.025*\"'see',\" + 0.023*\"'u',\" + 0.023*\"'love',\" + 0.022*\"'entrant']\" + 0.021*\"'comment',\"\n",
      "\n",
      "Score: 0.016667524352669716\t \n",
      "Topic: 0.065*\"'u',\" + 0.046*\"'tell',\" + 0.043*\"'dot',\" + 0.043*\"'green',\" + 0.038*\"'comment']\" + 0.030*\"'get',\" + 0.025*\"'back',\" + 0.025*\"'see',\" + 0.022*\"'weekend',\" + 0.019*\"'tax',\"\n",
      "\n",
      "Score: 0.016667494550347328\t \n",
      "Topic: 0.046*\"'card',\" + 0.043*\"'win',\" + 0.030*\"'fee',\" + 0.028*\"'cash',\" + 0.026*\"'chance',\" + 0.024*\"'dot',\" + 0.023*\"'back',\" + 0.023*\"'green',\" + 0.023*\"'apply',\" + 0.022*\"'tax',\"\n",
      "\n",
      "Score: 0.016667217016220093\t \n",
      "Topic: 0.054*\"'u',\" + 0.047*\"'tip',\" + 0.047*\"'like',\" + 0.038*\"'comment',\" + 0.034*\"'see',\" + 0.030*\"'sweepstakes',\" + 0.030*\"'financial',\" + 0.027*\"'page',\" + 0.025*\"'tell',\" + 0.022*\"'win',\"\n",
      "\n",
      "Score: 0.01666702702641487\t \n",
      "Topic: 0.083*\"'get',\" + 0.063*\"'pay',\" + 0.050*\"'day',\" + 0.040*\"'period',\" + 0.040*\"'deposit',\" + 0.024*\"'like',\" + 0.023*\"'direct',\" + 0.023*\"'employer',\" + 0.022*\"'may',\" + 0.022*\"'notice',\"\n",
      "\n",
      "Score: 0.01666688360273838\t \n",
      "Topic: 0.057*\"'card',\" + 0.044*\"'money',\" + 0.036*\"'learn',\" + 0.030*\"'save',\" + 0.029*\"'limit',\" + 0.027*\"'cash',\" + 0.026*\"'moneypak',\" + 0.025*\"'green',\" + 0.025*\"'dot',\" + 0.024*\"'share',\"\n",
      "\n",
      "Score: 0.016666876152157784\t \n",
      "Topic: 0.045*\"'direct',\" + 0.042*\"'get',\" + 0.042*\"'asap',\" + 0.040*\"'deposit',\" + 0.036*\"'paid',\" + 0.032*\"'payment',\" + 0.031*\"'finance',\" + 0.031*\"'win',\" + 0.030*\"'day',\" + 0.030*\"'play',\"\n",
      "\n",
      "Score: 0.016666708514094353\t \n",
      "Topic: 0.086*\"'back',\" + 0.083*\"'cash',\" + 0.054*\"'green',\" + 0.053*\"'dot',\" + 0.050*\"'card',\" + 0.044*\"'visa',\" + 0.042*\"'debit',\" + 0.042*\"'fee',\" + 0.038*\"'annually',\" + 0.026*\"'get',\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model[bow_corpus[500]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance evaluation by classifying sample document using LDA TF-IDF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.8499804139137268\t \n",
      "Topic: 0.049*\"'back',\" + 0.042*\"'cash',\" + 0.034*\"'love',\" + 0.028*\"'visa',\" + 0.026*\"'green',\" + 0.026*\"'dot',\" + 0.026*\"'card',\" + 0.025*\"'debit',\" + 0.025*\"'favorite',\" + 0.023*\"'bonus',\"\n",
      "\n",
      "Score: 0.016671495512127876\t \n",
      "Topic: 0.063*\"'payment',\" + 0.060*\"'ssi',\" + 0.034*\"'stay',\" + 0.032*\"'see',\" + 0.031*\"'tip',\" + 0.029*\"'benefit',\" + 0.029*\"'time',\" + 0.028*\"'person']\" + 0.024*\"'week',\" + 0.023*\"'someone',\"\n",
      "\n",
      "Score: 0.01667030341923237\t \n",
      "Topic: 0.060*\"'money',\" + 0.051*\"'read',\" + 0.049*\"'save',\" + 0.043*\"'ontheblog']\" + 0.039*\"'finance',\" + 0.031*\"'financial',\" + 0.025*\"['learn',\" + 0.025*\"'way',\" + 0.025*\"'read']\" + 0.021*\"'u',\"\n",
      "\n",
      "Score: 0.01667006127536297\t \n",
      "Topic: 0.035*\"'way',\" + 0.033*\"'advice',\" + 0.033*\"'green',\" + 0.030*\"'dot',\" + 0.029*\"'app',\" + 0.025*\"'mobile',\" + 0.023*\"'win',\" + 0.022*\"'learn']\" + 0.021*\"'sweepstakes',\" + 0.020*\"'winner']\"\n",
      "\n",
      "Score: 0.016669511795043945\t \n",
      "Topic: 0.097*\"'financial',\" + 0.037*\"'goal',\" + 0.030*\"'new',\" + 0.029*\"'comment']\" + 0.028*\"'best',\" + 0.027*\"'someone',\" + 0.027*\"'credit',\" + 0.025*\"['learn',\" + 0.024*\"'one',\" + 0.024*\"'learn',\"\n",
      "\n",
      "Score: 0.01666848547756672\t \n",
      "Topic: 0.040*\"'share',\" + 0.038*\"'mondaymotivation']\" + 0.029*\"'day',\" + 0.028*\"'plan',\" + 0.025*\"'extra',\" + 0.024*\"'customer',\" + 0.024*\"'pay',\" + 0.022*\"'entrant',\" + 0.022*\"'reflects',\" + 0.022*\"'statement',\"\n",
      "\n",
      "Score: 0.016667909920215607\t \n",
      "Topic: 0.047*\"'u',\" + 0.046*\"'tell',\" + 0.038*\"'day',\" + 0.035*\"'credit',\" + 0.027*\"'moneypak',\" + 0.026*\"'card',\" + 0.023*\"'check',\" + 0.023*\"'get',\" + 0.020*\"'comment',\" + 0.018*\"'using',\"\n",
      "\n",
      "Score: 0.016667502000927925\t \n",
      "Topic: 0.035*\"'tip',\" + 0.035*\"'budget',\" + 0.026*\"'back',\" + 0.025*\"'like',\" + 0.023*\"'u',\" + 0.023*\"'saving',\" + 0.023*\"'page',\" + 0.022*\"'cash',\" + 0.022*\"'sweepstakes',\" + 0.022*\"'let',\"\n",
      "\n",
      "Score: 0.016667349264025688\t \n",
      "Topic: 0.056*\"['green',\" + 0.034*\"'tax',\" + 0.029*\"'comment',\" + 0.027*\"'dot',\" + 0.027*\"'money',\" + 0.022*\"'tip']\" + 0.021*\"'prize',\" + 0.021*\"'finance',\" + 0.021*\"'smart',\" + 0.021*\"'giving',\"\n",
      "\n",
      "Score: 0.01666698046028614\t \n",
      "Topic: 0.046*\"['happy',\" + 0.036*\"'family',\" + 0.033*\"'make',\" + 0.031*\"'balance',\" + 0.031*\"'financialliteracymonth']\" + 0.024*\"'green',\" + 0.023*\"'card',\" + 0.021*\"'like',\" + 0.020*\"'account',\" + 0.019*\"'fee',\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model_tfidf[bow_corpus[500]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model_tfidf.print_topic(index, 10)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
