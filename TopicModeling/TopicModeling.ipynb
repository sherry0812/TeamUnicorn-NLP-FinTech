{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling Using LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description: Topic Modeling is a type of statistical modeling for discovering the abstract topics that occur in a collection of documents.\n",
    "\n",
    "Latent Dirichlet Allocation (LDA) is an example of topic model and is used to classify text in a document to a particular topic. It builds a topic per document model and words per topic model, modeled as Dirichlet distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora, models\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>by</th>\n",
       "      <th>category</th>\n",
       "      <th>comment_likes_count</th>\n",
       "      <th>comments_base</th>\n",
       "      <th>comments_count_fb</th>\n",
       "      <th>comments_replies</th>\n",
       "      <th>comments_retrieved</th>\n",
       "      <th>engagement_fb</th>\n",
       "      <th>likes_count_fb</th>\n",
       "      <th>post_message</th>\n",
       "      <th>post_published</th>\n",
       "      <th>rea_ANGRY</th>\n",
       "      <th>rea_HAHA</th>\n",
       "      <th>rea_LOVE</th>\n",
       "      <th>rea_SAD</th>\n",
       "      <th>rea_THANKFUL</th>\n",
       "      <th>rea_WOW</th>\n",
       "      <th>reactions_count_fb</th>\n",
       "      <th>shares_count_fb</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>post_page_155027942462</td>\n",
       "      <td>App Update</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>11</td>\n",
       "      <td>['paying', 'bill', 'autopay', 'ensures', 'bill...</td>\n",
       "      <td>2019-06-27T14:30:13+0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>photo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>post_page_155027942462</td>\n",
       "      <td>Engagement</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>110</td>\n",
       "      <td>70</td>\n",
       "      <td>['raise', 'hand', 'excited', 'first', 'day', '...</td>\n",
       "      <td>2019-06-21T14:31:06+0000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>76</td>\n",
       "      <td>4</td>\n",
       "      <td>photo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>post_page_155027942462</td>\n",
       "      <td>Engagement</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>5</td>\n",
       "      <td>['couple', 'save', 'together', 'stay', 'foreve...</td>\n",
       "      <td>2019-06-20T15:00:28+0000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>photo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>post_page_155027942462</td>\n",
       "      <td>Engagement</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>8</td>\n",
       "      <td>['case', 'forgotten', 'make', 'saving', 'daily...</td>\n",
       "      <td>2019-06-17T14:30:11+0000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>photo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>post_page_155027942462</td>\n",
       "      <td>Engagement</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>6</td>\n",
       "      <td>['father', 'day', 'approaching', 'got', 'smart...</td>\n",
       "      <td>2019-06-14T14:30:02+0000</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>video</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       by    category  comment_likes_count  comments_base  \\\n",
       "0  post_page_155027942462  App Update                    0              0   \n",
       "1  post_page_155027942462  Engagement                    0              0   \n",
       "2  post_page_155027942462  Engagement                    0              0   \n",
       "3  post_page_155027942462  Engagement                    0              0   \n",
       "4  post_page_155027942462  Engagement                    0              0   \n",
       "\n",
       "   comments_count_fb  comments_replies  comments_retrieved  engagement_fb  \\\n",
       "0                 11                 0                   0             22   \n",
       "1                 30                 0                   0            110   \n",
       "2                 11                 0                   0             17   \n",
       "3                 10                 0                   0             19   \n",
       "4                  8                 0                   0             17   \n",
       "\n",
       "   likes_count_fb                                       post_message  \\\n",
       "0              11  ['paying', 'bill', 'autopay', 'ensures', 'bill...   \n",
       "1              70  ['raise', 'hand', 'excited', 'first', 'day', '...   \n",
       "2               5  ['couple', 'save', 'together', 'stay', 'foreve...   \n",
       "3               8  ['case', 'forgotten', 'make', 'saving', 'daily...   \n",
       "4               6  ['father', 'day', 'approaching', 'got', 'smart...   \n",
       "\n",
       "             post_published  rea_ANGRY  rea_HAHA  rea_LOVE  rea_SAD  \\\n",
       "0  2019-06-27T14:30:13+0000          0         0         0        0   \n",
       "1  2019-06-21T14:31:06+0000          1         0         4        0   \n",
       "2  2019-06-20T15:00:28+0000          1         0         0        0   \n",
       "3  2019-06-17T14:30:11+0000          1         0         0        0   \n",
       "4  2019-06-14T14:30:02+0000          3         0         0        0   \n",
       "\n",
       "   rea_THANKFUL  rea_WOW  reactions_count_fb  shares_count_fb   type  \n",
       "0             0        0                  11                0  photo  \n",
       "1             0        1                  76                4  photo  \n",
       "2             0        0                   6                0  photo  \n",
       "3             0        0                   9                0  photo  \n",
       "4             0        0                   9                0  video  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv('fullstatCleaned_withLabels.tsv', sep='\\t')\n",
    "# Inspect df\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selct necessary columns\n",
    "processed_words = df['post_message']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words on the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tokenized string for BoW input\n",
    "processed_words = [words.split() for words in processed_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary from ‘processed_words’ containing the number of times a word appears in the training set\n",
    "dictionary = gensim.corpora.Dictionary(processed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out tokens\n",
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary reporting how many words and how many times those words appear\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 48 (\"'payday',\") appears 1 time.\n",
      "Word 60 (\"'see',\") appears 1 time.\n",
      "Word 92 (\"'week',\") appears 1 time.\n",
      "Word 102 (\"'bonus',\") appears 1 time.\n",
      "Word 148 (\"'winner']\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "# Preview Bag of Words for a sample preprocessed document\n",
    "bow_doc_500 = bow_corpus[500]\n",
    "\n",
    "for i in range(len(bow_doc_500)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_500[i][0], \n",
    "                                                     dictionary[bow_doc_500[i][0]], \n",
    "                                                     bow_doc_500[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tf-idf model object\n",
    "tfidf = models.TfidfModel(bow_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply transformation to the entire corpus\n",
    "corpus_tfidf = tfidf[bow_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.8509381336929653), (1, 0.40105490505841873), (2, 0.33920385575007195)]\n"
     ]
    }
   ],
   "source": [
    "# Preview TF-IDF scores for the first document\n",
    "for doc in corpus_tfidf:\n",
    "    pprint(doc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running LDA using Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the lda model using gensim.models.LdaMulticore and bow_corpus\n",
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=10, id2word=dictionary, passes=2, workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.065*\"'payday',\" + 0.060*\"'bonus',\" + 0.044*\"'extra',\" + 0.040*\"'payment',\" + 0.039*\"'entrant',\" + 0.037*\"'customer',\" + 0.034*\"'statement',\" + 0.034*\"'reflects',\" + 0.034*\"'ssi',\" + 0.032*\"'experience']\"\n",
      "Topic: 1 \n",
      "Words: 0.085*\"'cash',\" + 0.074*\"'back',\" + 0.055*\"'card',\" + 0.048*\"'dot',\" + 0.046*\"'green',\" + 0.043*\"'money',\" + 0.039*\"'visa',\" + 0.037*\"'debit',\" + 0.036*\"'earn',\" + 0.028*\"'annually',\"\n",
      "Topic: 2 \n",
      "Words: 0.076*\"'green',\" + 0.070*\"'dot',\" + 0.051*\"'money',\" + 0.030*\"'app',\" + 0.030*\"'read',\" + 0.028*\"'mobile',\" + 0.026*\"'u',\" + 0.025*\"'card',\" + 0.024*\"'balance',\" + 0.023*\"'learn']\"\n",
      "Topic: 3 \n",
      "Words: 0.036*\"'family',\" + 0.034*\"'play',\" + 0.032*\"'win',\" + 0.030*\"'back',\" + 0.029*\"'spin',\" + 0.028*\"'day',\" + 0.027*\"'cash',\" + 0.026*\"'get',\" + 0.025*\"'game',\" + 0.022*\"'tip',\"\n",
      "Topic: 4 \n",
      "Words: 0.081*\"'u',\" + 0.060*\"'financial',\" + 0.049*\"'tell',\" + 0.041*\"'make',\" + 0.036*\"'tip',\" + 0.031*\"'finance',\" + 0.029*\"'comment']\" + 0.028*\"'stay',\" + 0.028*\"'ontheblog']\" + 0.024*\"'like',\"\n",
      "Topic: 5 \n",
      "Words: 0.058*\"'sweepstakes',\" + 0.050*\"'like',\" + 0.043*\"'win',\" + 0.038*\"'green',\" + 0.036*\"'dot',\" + 0.032*\"'u',\" + 0.032*\"'page',\" + 0.032*\"'see',\" + 0.032*\"'time',\" + 0.031*\"'chance',\"\n",
      "Topic: 6 \n",
      "Words: 0.065*\"'tax',\" + 0.035*\"'dot',\" + 0.033*\"'card',\" + 0.031*\"'refund',\" + 0.031*\"'green',\" + 0.029*\"'win',\" + 0.026*\"'see',\" + 0.025*\"'prize',\" + 0.025*\"'deposit',\" + 0.023*\"'direct',\"\n",
      "Topic: 7 \n",
      "Words: 0.087*\"'card',\" + 0.054*\"'dot',\" + 0.042*\"'moneypak',\" + 0.036*\"['green',\" + 0.032*\"'prepaid',\" + 0.031*\"'credit',\" + 0.027*\"'green',\" + 0.027*\"'debit',\" + 0.027*\"'learn',\" + 0.026*\"'service',\"\n",
      "Topic: 8 \n",
      "Words: 0.069*\"'see',\" + 0.046*\"'comment',\" + 0.043*\"'week',\" + 0.039*\"'sweepstakes',\" + 0.039*\"'bonus',\" + 0.038*\"'payday',\" + 0.032*\"'winner',\" + 0.025*\"'dot',\" + 0.025*\"'green',\" + 0.025*\"'share',\"\n",
      "Topic: 9 \n",
      "Words: 0.067*\"'get',\" + 0.049*\"'card',\" + 0.044*\"'green',\" + 0.043*\"'dot',\" + 0.038*\"'back',\" + 0.037*\"'fee',\" + 0.036*\"'cash',\" + 0.036*\"'pay',\" + 0.033*\"'deposit',\" + 0.022*\"'visa',\"\n"
     ]
    }
   ],
   "source": [
    "# For each topic, explore the words occuring in that topic and its relative weight.\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running LDA using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the lda model using gensim.models.LdaMulticore and corpus_tfidf\n",
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=10, id2word=dictionary, passes=2, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 Word: 0.059*\"'financial',\" + 0.040*\"'week',\" + 0.038*\"'bonus',\" + 0.037*\"'payday',\" + 0.031*\"'see',\" + 0.029*\"'someone',\" + 0.029*\"'balance',\" + 0.028*\"'check',\" + 0.028*\"'day',\" + 0.027*\"'winner',\"\n",
      "Topic: 1 Word: 0.041*\"'tell',\" + 0.038*\"'comment']\" + 0.035*\"'u',\" + 0.028*\"'like',\" + 0.028*\"'summer',\" + 0.026*\"'sweepstakes',\" + 0.025*\"'tip',\" + 0.025*\"'comment',\" + 0.023*\"'page',\" + 0.023*\"'weekend',\"\n",
      "Topic: 2 Word: 0.082*\"['learn',\" + 0.055*\"'tip',\" + 0.048*\"'new',\" + 0.036*\"'save',\" + 0.032*\"'time',\" + 0.027*\"'using',\" + 0.024*\"'today',\" + 0.020*\"'every',\" + 0.020*\"'ontheblog']\" + 0.019*\"'credit',\"\n",
      "Topic: 3 Word: 0.057*\"'back',\" + 0.047*\"'cash',\" + 0.044*\"'get',\" + 0.035*\"['happy',\" + 0.032*\"'shopping',\" + 0.031*\"'visa',\" + 0.026*\"'card',\" + 0.026*\"'check',\" + 0.026*\"'annually',\" + 0.024*\"'green',\"\n",
      "Topic: 4 Word: 0.041*\"'credit',\" + 0.034*\"'like',\" + 0.031*\"'learn',\" + 0.029*\"'financialliteracymonth']\" + 0.028*\"'page',\" + 0.027*\"'back',\" + 0.025*\"'read']\" + 0.022*\"'budgetfriendly',\" + 0.022*\"'financial',\" + 0.020*\"'bill',\"\n",
      "Topic: 5 Word: 0.035*\"'see',\" + 0.035*\"'favorite',\" + 0.029*\"'read',\" + 0.027*\"'winner']\" + 0.025*\"'u',\" + 0.023*\"'make',\" + 0.023*\"'love',\" + 0.022*\"'comment',\" + 0.022*\"'dot',\" + 0.022*\"'week',\"\n",
      "Topic: 6 Word: 0.035*\"'card',\" + 0.028*\"'cash',\" + 0.028*\"'pay',\" + 0.027*\"'dot',\" + 0.026*\"'fee',\" + 0.026*\"'get',\" + 0.025*\"'green',\" + 0.025*\"'back',\" + 0.022*\"'family',\" + 0.022*\"'prepaid',\"\n",
      "Topic: 7 Word: 0.030*\"'cash',\" + 0.030*\"'tax',\" + 0.029*\"'back',\" + 0.029*\"'comment',\" + 0.029*\"'card',\" + 0.026*\"'mondaymotivation']\" + 0.026*\"'earn',\" + 0.022*\"'let',\" + 0.021*\"'visa',\" + 0.020*\"'know',\"\n",
      "Topic: 8 Word: 0.061*\"'money',\" + 0.056*\"'way',\" + 0.047*\"'share',\" + 0.044*\"['green',\" + 0.042*\"'u',\" + 0.040*\"'finance',\" + 0.036*\"'budget',\" + 0.029*\"'save',\" + 0.028*\"'stay',\" + 0.022*\"'saving',\"\n",
      "Topic: 9 Word: 0.050*\"'payment',\" + 0.045*\"'ssi',\" + 0.038*\"'advice',\" + 0.032*\"'bonus',\" + 0.031*\"'payday',\" + 0.029*\"'love',\" + 0.027*\"'entrant',\" + 0.026*\"'benefit',\" + 0.026*\"'sweepstakes',\" + 0.025*\"'customer',\"\n"
     ]
    }
   ],
   "source": [
    "# For each topic, explore the words occuring in that topic and its relative weight.\n",
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification of the Topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance evaluation by classifying sample document using LDA Bag of Words model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"['someone',\",\n",
       " \"'cloud',\",\n",
       " \"'nine',\",\n",
       " \"'see',\",\n",
       " \"'week',\",\n",
       " \"'payday',\",\n",
       " \"'bonus',\",\n",
       " \"'winner']\"]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_words[500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.8499889969825745\t \n",
      "Topic: 0.069*\"'see',\" + 0.046*\"'comment',\" + 0.043*\"'week',\" + 0.039*\"'sweepstakes',\" + 0.039*\"'bonus',\" + 0.038*\"'payday',\" + 0.032*\"'winner',\" + 0.025*\"'dot',\" + 0.025*\"'green',\" + 0.025*\"'share',\"\n",
      "\n",
      "Score: 0.016671868041157722\t \n",
      "Topic: 0.065*\"'payday',\" + 0.060*\"'bonus',\" + 0.044*\"'extra',\" + 0.040*\"'payment',\" + 0.039*\"'entrant',\" + 0.037*\"'customer',\" + 0.034*\"'statement',\" + 0.034*\"'reflects',\" + 0.034*\"'ssi',\" + 0.032*\"'experience']\"\n",
      "\n",
      "Score: 0.016668537631630898\t \n",
      "Topic: 0.065*\"'tax',\" + 0.035*\"'dot',\" + 0.033*\"'card',\" + 0.031*\"'refund',\" + 0.031*\"'green',\" + 0.029*\"'win',\" + 0.026*\"'see',\" + 0.025*\"'prize',\" + 0.025*\"'deposit',\" + 0.023*\"'direct',\"\n",
      "\n",
      "Score: 0.016668010503053665\t \n",
      "Topic: 0.036*\"'family',\" + 0.034*\"'play',\" + 0.032*\"'win',\" + 0.030*\"'back',\" + 0.029*\"'spin',\" + 0.028*\"'day',\" + 0.027*\"'cash',\" + 0.026*\"'get',\" + 0.025*\"'game',\" + 0.022*\"'tip',\"\n",
      "\n",
      "Score: 0.01666739396750927\t \n",
      "Topic: 0.076*\"'green',\" + 0.070*\"'dot',\" + 0.051*\"'money',\" + 0.030*\"'app',\" + 0.030*\"'read',\" + 0.028*\"'mobile',\" + 0.026*\"'u',\" + 0.025*\"'card',\" + 0.024*\"'balance',\" + 0.023*\"'learn']\"\n",
      "\n",
      "Score: 0.016667362302541733\t \n",
      "Topic: 0.058*\"'sweepstakes',\" + 0.050*\"'like',\" + 0.043*\"'win',\" + 0.038*\"'green',\" + 0.036*\"'dot',\" + 0.032*\"'u',\" + 0.032*\"'page',\" + 0.032*\"'see',\" + 0.032*\"'time',\" + 0.031*\"'chance',\"\n",
      "\n",
      "Score: 0.016667049378156662\t \n",
      "Topic: 0.081*\"'u',\" + 0.060*\"'financial',\" + 0.049*\"'tell',\" + 0.041*\"'make',\" + 0.036*\"'tip',\" + 0.031*\"'finance',\" + 0.029*\"'comment']\" + 0.028*\"'stay',\" + 0.028*\"'ontheblog']\" + 0.024*\"'like',\"\n",
      "\n",
      "Score: 0.01666698046028614\t \n",
      "Topic: 0.067*\"'get',\" + 0.049*\"'card',\" + 0.044*\"'green',\" + 0.043*\"'dot',\" + 0.038*\"'back',\" + 0.037*\"'fee',\" + 0.036*\"'cash',\" + 0.036*\"'pay',\" + 0.033*\"'deposit',\" + 0.022*\"'visa',\"\n",
      "\n",
      "Score: 0.016666941344738007\t \n",
      "Topic: 0.087*\"'card',\" + 0.054*\"'dot',\" + 0.042*\"'moneypak',\" + 0.036*\"['green',\" + 0.032*\"'prepaid',\" + 0.031*\"'credit',\" + 0.027*\"'green',\" + 0.027*\"'debit',\" + 0.027*\"'learn',\" + 0.026*\"'service',\"\n",
      "\n",
      "Score: 0.016666855663061142\t \n",
      "Topic: 0.085*\"'cash',\" + 0.074*\"'back',\" + 0.055*\"'card',\" + 0.048*\"'dot',\" + 0.046*\"'green',\" + 0.043*\"'money',\" + 0.039*\"'visa',\" + 0.037*\"'debit',\" + 0.036*\"'earn',\" + 0.028*\"'annually',\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model[bow_corpus[500]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance evaluation by classifying sample document using LDA TF-IDF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.8499898314476013\t \n",
      "Topic: 0.059*\"'financial',\" + 0.040*\"'week',\" + 0.038*\"'bonus',\" + 0.037*\"'payday',\" + 0.031*\"'see',\" + 0.029*\"'someone',\" + 0.029*\"'balance',\" + 0.028*\"'check',\" + 0.028*\"'day',\" + 0.027*\"'winner',\"\n",
      "\n",
      "Score: 0.016670944169163704\t \n",
      "Topic: 0.035*\"'see',\" + 0.035*\"'favorite',\" + 0.029*\"'read',\" + 0.027*\"'winner']\" + 0.025*\"'u',\" + 0.023*\"'make',\" + 0.023*\"'love',\" + 0.022*\"'comment',\" + 0.022*\"'dot',\" + 0.022*\"'week',\"\n",
      "\n",
      "Score: 0.01666988432407379\t \n",
      "Topic: 0.050*\"'payment',\" + 0.045*\"'ssi',\" + 0.038*\"'advice',\" + 0.032*\"'bonus',\" + 0.031*\"'payday',\" + 0.029*\"'love',\" + 0.027*\"'entrant',\" + 0.026*\"'benefit',\" + 0.026*\"'sweepstakes',\" + 0.025*\"'customer',\"\n",
      "\n",
      "Score: 0.016667841002345085\t \n",
      "Topic: 0.041*\"'tell',\" + 0.038*\"'comment']\" + 0.035*\"'u',\" + 0.028*\"'like',\" + 0.028*\"'summer',\" + 0.026*\"'sweepstakes',\" + 0.025*\"'tip',\" + 0.025*\"'comment',\" + 0.023*\"'page',\" + 0.023*\"'weekend',\"\n",
      "\n",
      "Score: 0.016667108982801437\t \n",
      "Topic: 0.082*\"['learn',\" + 0.055*\"'tip',\" + 0.048*\"'new',\" + 0.036*\"'save',\" + 0.032*\"'time',\" + 0.027*\"'using',\" + 0.024*\"'today',\" + 0.020*\"'every',\" + 0.020*\"'ontheblog']\" + 0.019*\"'credit',\"\n",
      "\n",
      "Score: 0.016667069867253304\t \n",
      "Topic: 0.057*\"'back',\" + 0.047*\"'cash',\" + 0.044*\"'get',\" + 0.035*\"['happy',\" + 0.032*\"'shopping',\" + 0.031*\"'visa',\" + 0.026*\"'card',\" + 0.026*\"'check',\" + 0.026*\"'annually',\" + 0.024*\"'green',\"\n",
      "\n",
      "Score: 0.016666896641254425\t \n",
      "Topic: 0.030*\"'cash',\" + 0.030*\"'tax',\" + 0.029*\"'back',\" + 0.029*\"'comment',\" + 0.029*\"'card',\" + 0.026*\"'mondaymotivation']\" + 0.026*\"'earn',\" + 0.022*\"'let',\" + 0.021*\"'visa',\" + 0.020*\"'know',\"\n",
      "\n",
      "Score: 0.01666688546538353\t \n",
      "Topic: 0.035*\"'card',\" + 0.028*\"'cash',\" + 0.028*\"'pay',\" + 0.027*\"'dot',\" + 0.026*\"'fee',\" + 0.026*\"'get',\" + 0.025*\"'green',\" + 0.025*\"'back',\" + 0.022*\"'family',\" + 0.022*\"'prepaid',\"\n",
      "\n",
      "Score: 0.0166668388992548\t \n",
      "Topic: 0.061*\"'money',\" + 0.056*\"'way',\" + 0.047*\"'share',\" + 0.044*\"['green',\" + 0.042*\"'u',\" + 0.040*\"'finance',\" + 0.036*\"'budget',\" + 0.029*\"'save',\" + 0.028*\"'stay',\" + 0.022*\"'saving',\"\n",
      "\n",
      "Score: 0.016666710376739502\t \n",
      "Topic: 0.041*\"'credit',\" + 0.034*\"'like',\" + 0.031*\"'learn',\" + 0.029*\"'financialliteracymonth']\" + 0.028*\"'page',\" + 0.027*\"'back',\" + 0.025*\"'read']\" + 0.022*\"'budgetfriendly',\" + 0.022*\"'financial',\" + 0.020*\"'bill',\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model_tfidf[bow_corpus[500]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model_tfidf.print_topic(index, 10)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
