{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling Using LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description: Topic Modeling is a type of statistical modeling for discovering the abstract topics that occur in a collection of documents.\n",
    "\n",
    "Latent Dirichlet Allocation (LDA) is an example of topic model and is used to classify text in a document to a particular topic. It builds a topic per document model and words per topic model, modeled as Dirichlet distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora, models\n",
    "from gensim.models import CoherenceModel\n",
    "from pprint import pprint\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>by</th>\n",
       "      <th>category</th>\n",
       "      <th>comment_likes_count</th>\n",
       "      <th>comments_base</th>\n",
       "      <th>comments_count_fb</th>\n",
       "      <th>comments_replies</th>\n",
       "      <th>comments_retrieved</th>\n",
       "      <th>engagement_fb</th>\n",
       "      <th>likes_count_fb</th>\n",
       "      <th>post_id</th>\n",
       "      <th>...</th>\n",
       "      <th>post_published</th>\n",
       "      <th>rea_ANGRY</th>\n",
       "      <th>rea_HAHA</th>\n",
       "      <th>rea_LOVE</th>\n",
       "      <th>rea_SAD</th>\n",
       "      <th>rea_THANKFUL</th>\n",
       "      <th>rea_WOW</th>\n",
       "      <th>reactions_count_fb</th>\n",
       "      <th>shares_count_fb</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>post_page_155027942462</td>\n",
       "      <td>App Update</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>11</td>\n",
       "      <td>155027942462_10157348020627463</td>\n",
       "      <td>...</td>\n",
       "      <td>2019-06-27T14:30:13+0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>photo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>post_page_155027942462</td>\n",
       "      <td>Engagement</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>110</td>\n",
       "      <td>70</td>\n",
       "      <td>155027942462_10157333387457463</td>\n",
       "      <td>...</td>\n",
       "      <td>2019-06-21T14:31:06+0000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>76</td>\n",
       "      <td>4</td>\n",
       "      <td>photo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>post_page_155027942462</td>\n",
       "      <td>Engagement</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>5</td>\n",
       "      <td>155027942462_10157330985232463</td>\n",
       "      <td>...</td>\n",
       "      <td>2019-06-20T15:00:28+0000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>photo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>post_page_155027942462</td>\n",
       "      <td>Engagement</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>8</td>\n",
       "      <td>155027942462_10157323881577463</td>\n",
       "      <td>...</td>\n",
       "      <td>2019-06-17T14:30:11+0000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>photo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>post_page_155027942462</td>\n",
       "      <td>Engagement</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>6</td>\n",
       "      <td>155027942462_10157315990422463</td>\n",
       "      <td>...</td>\n",
       "      <td>2019-06-14T14:30:02+0000</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>video</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       by    category  comment_likes_count  comments_base  \\\n",
       "0  post_page_155027942462  App Update                    0              0   \n",
       "1  post_page_155027942462  Engagement                    0              0   \n",
       "2  post_page_155027942462  Engagement                    0              0   \n",
       "3  post_page_155027942462  Engagement                    0              0   \n",
       "4  post_page_155027942462  Engagement                    0              0   \n",
       "\n",
       "   comments_count_fb  comments_replies  comments_retrieved  engagement_fb  \\\n",
       "0                 11                 0                   0             22   \n",
       "1                 30                 0                   0            110   \n",
       "2                 11                 0                   0             17   \n",
       "3                 10                 0                   0             19   \n",
       "4                  8                 0                   0             17   \n",
       "\n",
       "   likes_count_fb                         post_id  ...    \\\n",
       "0              11  155027942462_10157348020627463  ...     \n",
       "1              70  155027942462_10157333387457463  ...     \n",
       "2               5  155027942462_10157330985232463  ...     \n",
       "3               8  155027942462_10157323881577463  ...     \n",
       "4               6  155027942462_10157315990422463  ...     \n",
       "\n",
       "             post_published rea_ANGRY  rea_HAHA  rea_LOVE  rea_SAD  \\\n",
       "0  2019-06-27T14:30:13+0000         0         0         0        0   \n",
       "1  2019-06-21T14:31:06+0000         1         0         4        0   \n",
       "2  2019-06-20T15:00:28+0000         1         0         0        0   \n",
       "3  2019-06-17T14:30:11+0000         1         0         0        0   \n",
       "4  2019-06-14T14:30:02+0000         3         0         0        0   \n",
       "\n",
       "   rea_THANKFUL  rea_WOW  reactions_count_fb  shares_count_fb   type  \n",
       "0             0        0                  11                0  photo  \n",
       "1             0        1                  76                4  photo  \n",
       "2             0        0                   6                0  photo  \n",
       "3             0        0                   9                0  photo  \n",
       "4             0        0                   9                0  video  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv('C:/Users/cherryb/Desktop/Personal Projects/Datasets/Telus - Fintech/cleaned/fullstatCleaned_withLabels.tsv', index_col='Unnamed: 0', sep='\\t')\n",
    "# Inspect df\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize post_message column\n",
    "df['post_message'] = df['post_message'].apply(lambda list_words: word_tokenize(list_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select df['post_message']\n",
    "processed_words = df['post_message']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of processed_words\n",
    "processed_words = [words for words in processed_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Word (BoW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary from ‘processed_words’ containing the number of times a word appears in the training set\n",
    "dictionary = corpora.Dictionary(processed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out tokens\n",
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary reporting how many words and how many times those words appear\n",
    "corpus_bow = [dictionary.doc2bow(doc) for doc in processed_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 57 (\"paydai\") appears 1 time.\n",
      "Word 68 (\"see\") appears 1 time.\n",
      "Word 108 (\"week\") appears 1 time.\n",
      "Word 118 (\"someon\") appears 1 time.\n",
      "Word 119 (\"bonu\") appears 1 time.\n",
      "Word 163 (\"winner\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "# Preview Bag of Words for a sample preprocessed document\n",
    "bow_doc_500 = corpus_bow[500]\n",
    "\n",
    "for i in range(len(bow_doc_500)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_500[i][0], dictionary[bow_doc_500[i][0]], bow_doc_500[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model\n",
    "tfidf = models.TfidfModel(corpus_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply transformation to the entire corpus\n",
    "corpus_tfidf = tfidf[corpus_bow]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.8057762139865111),\n",
      " (1, 0.3489304433755792),\n",
      " (2, 0.39392744378681527),\n",
      " (3, 0.2716494205605292)]\n"
     ]
    }
   ],
   "source": [
    "# Preview TF-IDF scores for the first document\n",
    "for doc in corpus_tfidf:\n",
    "    pprint(doc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling - LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the lda model using gensim.models.LdaMulticore and corpus_bow\n",
    "lda_model = gensim.models.LdaMulticore(corpus_bow, num_topics=10, id2word=dictionary, passes=2, workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.039*\"credit\" + 0.035*\"wai\" + 0.033*\"save\" + 0.028*\"u\" + 0.025*\"make\" + 0.025*\"monei\" + 0.023*\"dai\" + 0.023*\"learn\" + 0.022*\"read\" + 0.022*\"comment\"\n",
      "\n",
      "Topic: 1 \n",
      "Words: 0.042*\"green\" + 0.042*\"get\" + 0.041*\"dot\" + 0.033*\"card\" + 0.031*\"sweepstak\" + 0.028*\"advic\" + 0.023*\"back\" + 0.020*\"cash\" + 0.018*\"help\" + 0.018*\"monei\"\n",
      "\n",
      "Topic: 2 \n",
      "Words: 0.040*\"app\" + 0.039*\"u\" + 0.039*\"mobil\" + 0.034*\"balanc\" + 0.029*\"account\" + 0.029*\"dot\" + 0.029*\"green\" + 0.028*\"tell\" + 0.028*\"learn\" + 0.021*\"bank\"\n",
      "\n",
      "Topic: 3 \n",
      "Words: 0.063*\"green\" + 0.061*\"dot\" + 0.048*\"tip\" + 0.041*\"card\" + 0.036*\"tax\" + 0.030*\"like\" + 0.026*\"love\" + 0.020*\"page\" + 0.018*\"season\" + 0.016*\"enter\"\n",
      "\n",
      "Topic: 4 \n",
      "Words: 0.073*\"card\" + 0.063*\"cash\" + 0.049*\"back\" + 0.045*\"dot\" + 0.044*\"green\" + 0.042*\"appli\" + 0.040*\"fee\" + 0.033*\"debit\" + 0.029*\"visa\" + 0.027*\"get\"\n",
      "\n",
      "Topic: 5 \n",
      "Words: 0.035*\"see\" + 0.035*\"green\" + 0.033*\"win\" + 0.033*\"dot\" + 0.030*\"winner\" + 0.029*\"dai\" + 0.026*\"week\" + 0.025*\"tip\" + 0.023*\"plai\" + 0.022*\"get\"\n",
      "\n",
      "Topic: 6 \n",
      "Words: 0.048*\"share\" + 0.041*\"monei\" + 0.036*\"save\" + 0.036*\"comment\" + 0.035*\"learn\" + 0.033*\"financ\" + 0.026*\"u\" + 0.026*\"see\" + 0.025*\"win\" + 0.021*\"thank\"\n",
      "\n",
      "Topic: 7 \n",
      "Words: 0.054*\"see\" + 0.045*\"u\" + 0.043*\"comment\" + 0.040*\"bonu\" + 0.038*\"paydai\" + 0.033*\"week\" + 0.032*\"tell\" + 0.027*\"financi\" + 0.027*\"like\" + 0.024*\"winner\"\n",
      "\n",
      "Topic: 8 \n",
      "Words: 0.042*\"back\" + 0.038*\"card\" + 0.038*\"dot\" + 0.037*\"green\" + 0.037*\"financi\" + 0.034*\"cash\" + 0.028*\"paydai\" + 0.028*\"bonu\" + 0.023*\"visa\" + 0.023*\"entrant\"\n",
      "\n",
      "Topic: 9 \n",
      "Words: 0.110*\"get\" + 0.055*\"pai\" + 0.042*\"dai\" + 0.041*\"period\" + 0.038*\"deposit\" + 0.025*\"time\" + 0.024*\"mai\" + 0.024*\"depend\" + 0.023*\"employ\" + 0.022*\"like\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For each topic, explore the words occuring in that topic and its relative weight.\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}\\n'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the lda model using gensim.models.LdaMulticore and corpus_tfidf\n",
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=10, id2word=dictionary, passes=2, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Word: 0.033*\"payment\" + 0.033*\"back\" + 0.031*\"schedul\" + 0.031*\"learn\" + 0.028*\"ssi\" + 0.026*\"get\" + 0.025*\"cash\" + 0.022*\"card\" + 0.022*\"talk\" + 0.019*\"bill\"\n",
      "\n",
      "Topic: 1 \n",
      "Word: 0.038*\"summer\" + 0.029*\"dai\" + 0.025*\"ontheblog\" + 0.023*\"make\" + 0.020*\"start\" + 0.019*\"go\" + 0.018*\"famili\" + 0.017*\"card\" + 0.017*\"dot\" + 0.016*\"green\"\n",
      "\n",
      "Topic: 2 \n",
      "Word: 0.036*\"happi\" + 0.024*\"read\" + 0.024*\"stai\" + 0.021*\"dot\" + 0.021*\"green\" + 0.021*\"new\" + 0.020*\"want\" + 0.019*\"like\" + 0.019*\"kind\" + 0.017*\"sweepstak\"\n",
      "\n",
      "Topic: 3 \n",
      "Word: 0.043*\"tip\" + 0.028*\"monei\" + 0.027*\"good\" + 0.025*\"save\" + 0.024*\"extra\" + 0.023*\"smart\" + 0.023*\"entrant\" + 0.023*\"experi\" + 0.022*\"custom\" + 0.022*\"statement\"\n",
      "\n",
      "Topic: 4 \n",
      "Word: 0.041*\"comment\" + 0.038*\"share\" + 0.031*\"u\" + 0.029*\"budget\" + 0.024*\"tell\" + 0.021*\"save\" + 0.019*\"wai\" + 0.018*\"dai\" + 0.018*\"prepaid\" + 0.017*\"goal\"\n",
      "\n",
      "Topic: 5 \n",
      "Word: 0.056*\"financi\" + 0.044*\"green\" + 0.043*\"dot\" + 0.031*\"love\" + 0.029*\"advic\" + 0.026*\"credit\" + 0.026*\"shop\" + 0.025*\"ontheblog\" + 0.025*\"read\" + 0.025*\"monei\"\n",
      "\n",
      "Topic: 6 \n",
      "Word: 0.045*\"monei\" + 0.035*\"season\" + 0.026*\"see\" + 0.025*\"free\" + 0.024*\"save\" + 0.021*\"thank\" + 0.021*\"plan\" + 0.018*\"extra\" + 0.018*\"u\" + 0.018*\"credit\"\n",
      "\n",
      "Topic: 7 \n",
      "Word: 0.053*\"week\" + 0.049*\"paydai\" + 0.047*\"bonu\" + 0.037*\"see\" + 0.034*\"someon\" + 0.032*\"winner\" + 0.024*\"treat\" + 0.024*\"pai\" + 0.024*\"get\" + 0.023*\"person\"\n",
      "\n",
      "Topic: 8 \n",
      "Word: 0.046*\"cash\" + 0.045*\"back\" + 0.032*\"balanc\" + 0.032*\"card\" + 0.029*\"favorit\" + 0.028*\"debit\" + 0.026*\"visa\" + 0.025*\"annual\" + 0.025*\"purchas\" + 0.024*\"appli\"\n",
      "\n",
      "Topic: 9 \n",
      "Word: 0.036*\"tax\" + 0.031*\"mondaymotiv\" + 0.023*\"refund\" + 0.021*\"tip\" + 0.021*\"win\" + 0.020*\"home\" + 0.019*\"card\" + 0.018*\"deposit\" + 0.017*\"direct\" + 0.017*\"keep\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For each topic, explore the words occuring in that topic and its relative weight.\n",
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} \\nWord: {}\\n'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification of the Topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance evaluation by classifying sample document using LDA Bag of Words model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['someon', 'cloud', 'nine', 'see', 'week', 'paydai', 'bonu', 'winner']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_words[500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.8714189529418945\t \n",
      "Topic: 0.054*\"see\" + 0.045*\"u\" + 0.043*\"comment\" + 0.040*\"bonu\" + 0.038*\"paydai\" + 0.033*\"week\" + 0.032*\"tell\" + 0.027*\"financi\" + 0.027*\"like\" + 0.024*\"winner\"\n",
      "\n",
      "Score: 0.014288805425167084\t \n",
      "Topic: 0.035*\"see\" + 0.035*\"green\" + 0.033*\"win\" + 0.033*\"dot\" + 0.030*\"winner\" + 0.029*\"dai\" + 0.026*\"week\" + 0.025*\"tip\" + 0.023*\"plai\" + 0.022*\"get\"\n",
      "\n",
      "Score: 0.014287449419498444\t \n",
      "Topic: 0.042*\"back\" + 0.038*\"card\" + 0.038*\"dot\" + 0.037*\"green\" + 0.037*\"financi\" + 0.034*\"cash\" + 0.028*\"paydai\" + 0.028*\"bonu\" + 0.023*\"visa\" + 0.023*\"entrant\"\n",
      "\n",
      "Score: 0.014286912977695465\t \n",
      "Topic: 0.110*\"get\" + 0.055*\"pai\" + 0.042*\"dai\" + 0.041*\"period\" + 0.038*\"deposit\" + 0.025*\"time\" + 0.024*\"mai\" + 0.024*\"depend\" + 0.023*\"employ\" + 0.022*\"like\"\n",
      "\n",
      "Score: 0.014286698773503304\t \n",
      "Topic: 0.048*\"share\" + 0.041*\"monei\" + 0.036*\"save\" + 0.036*\"comment\" + 0.035*\"learn\" + 0.033*\"financ\" + 0.026*\"u\" + 0.026*\"see\" + 0.025*\"win\" + 0.021*\"thank\"\n",
      "\n",
      "Score: 0.01428657490760088\t \n",
      "Topic: 0.042*\"green\" + 0.042*\"get\" + 0.041*\"dot\" + 0.033*\"card\" + 0.031*\"sweepstak\" + 0.028*\"advic\" + 0.023*\"back\" + 0.020*\"cash\" + 0.018*\"help\" + 0.018*\"monei\"\n",
      "\n",
      "Score: 0.014286323450505733\t \n",
      "Topic: 0.039*\"credit\" + 0.035*\"wai\" + 0.033*\"save\" + 0.028*\"u\" + 0.025*\"make\" + 0.025*\"monei\" + 0.023*\"dai\" + 0.023*\"learn\" + 0.022*\"read\" + 0.022*\"comment\"\n",
      "\n",
      "Score: 0.014286195859313011\t \n",
      "Topic: 0.040*\"app\" + 0.039*\"u\" + 0.039*\"mobil\" + 0.034*\"balanc\" + 0.029*\"account\" + 0.029*\"dot\" + 0.029*\"green\" + 0.028*\"tell\" + 0.028*\"learn\" + 0.021*\"bank\"\n",
      "\n",
      "Score: 0.014286145567893982\t \n",
      "Topic: 0.063*\"green\" + 0.061*\"dot\" + 0.048*\"tip\" + 0.041*\"card\" + 0.036*\"tax\" + 0.030*\"like\" + 0.026*\"love\" + 0.020*\"page\" + 0.018*\"season\" + 0.016*\"enter\"\n",
      "\n",
      "Score: 0.01428594533354044\t \n",
      "Topic: 0.073*\"card\" + 0.063*\"cash\" + 0.049*\"back\" + 0.045*\"dot\" + 0.044*\"green\" + 0.042*\"appli\" + 0.040*\"fee\" + 0.033*\"debit\" + 0.029*\"visa\" + 0.027*\"get\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model[corpus_bow[500]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance evaluation by classifying sample document using LDA TF-IDF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.8714226484298706\t \n",
      "Topic: 0.053*\"week\" + 0.049*\"paydai\" + 0.047*\"bonu\" + 0.037*\"see\" + 0.034*\"someon\" + 0.032*\"winner\" + 0.024*\"treat\" + 0.024*\"pai\" + 0.024*\"get\" + 0.023*\"person\"\n",
      "\n",
      "Score: 0.01428710762411356\t \n",
      "Topic: 0.045*\"monei\" + 0.035*\"season\" + 0.026*\"see\" + 0.025*\"free\" + 0.024*\"save\" + 0.021*\"thank\" + 0.021*\"plan\" + 0.018*\"extra\" + 0.018*\"u\" + 0.018*\"credit\"\n",
      "\n",
      "Score: 0.014286509715020657\t \n",
      "Topic: 0.043*\"tip\" + 0.028*\"monei\" + 0.027*\"good\" + 0.025*\"save\" + 0.024*\"extra\" + 0.023*\"smart\" + 0.023*\"entrant\" + 0.023*\"experi\" + 0.022*\"custom\" + 0.022*\"statement\"\n",
      "\n",
      "Score: 0.014286419376730919\t \n",
      "Topic: 0.041*\"comment\" + 0.038*\"share\" + 0.031*\"u\" + 0.029*\"budget\" + 0.024*\"tell\" + 0.021*\"save\" + 0.019*\"wai\" + 0.018*\"dai\" + 0.018*\"prepaid\" + 0.017*\"goal\"\n",
      "\n",
      "Score: 0.01428639329969883\t \n",
      "Topic: 0.056*\"financi\" + 0.044*\"green\" + 0.043*\"dot\" + 0.031*\"love\" + 0.029*\"advic\" + 0.026*\"credit\" + 0.026*\"shop\" + 0.025*\"ontheblog\" + 0.025*\"read\" + 0.025*\"monei\"\n",
      "\n",
      "Score: 0.014286370016634464\t \n",
      "Topic: 0.038*\"summer\" + 0.029*\"dai\" + 0.025*\"ontheblog\" + 0.023*\"make\" + 0.020*\"start\" + 0.019*\"go\" + 0.018*\"famili\" + 0.017*\"card\" + 0.017*\"dot\" + 0.016*\"green\"\n",
      "\n",
      "Score: 0.014286323450505733\t \n",
      "Topic: 0.036*\"happi\" + 0.024*\"read\" + 0.024*\"stai\" + 0.021*\"dot\" + 0.021*\"green\" + 0.021*\"new\" + 0.020*\"want\" + 0.019*\"like\" + 0.019*\"kind\" + 0.017*\"sweepstak\"\n",
      "\n",
      "Score: 0.014286184683442116\t \n",
      "Topic: 0.036*\"tax\" + 0.031*\"mondaymotiv\" + 0.023*\"refund\" + 0.021*\"tip\" + 0.021*\"win\" + 0.020*\"home\" + 0.019*\"card\" + 0.018*\"deposit\" + 0.017*\"direct\" + 0.017*\"keep\"\n",
      "\n",
      "Score: 0.014286087825894356\t \n",
      "Topic: 0.046*\"cash\" + 0.045*\"back\" + 0.032*\"balanc\" + 0.032*\"card\" + 0.029*\"favorit\" + 0.028*\"debit\" + 0.026*\"visa\" + 0.025*\"annual\" + 0.025*\"purchas\" + 0.024*\"appli\"\n",
      "\n",
      "Score: 0.014285970479249954\t \n",
      "Topic: 0.033*\"payment\" + 0.033*\"back\" + 0.031*\"schedul\" + 0.031*\"learn\" + 0.028*\"ssi\" + 0.026*\"get\" + 0.025*\"cash\" + 0.022*\"card\" + 0.022*\"talk\" + 0.019*\"bill\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model_tfidf[corpus_bow[500]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model_tfidf.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the LDA Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"As a rule of thumb for a good LDA model, the perplexity score should be low while coherence should be high\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA using BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity: -4.999184949024548\n",
      "\n",
      "Coherence Score: 0.4279726410034108\n"
     ]
    }
   ],
   "source": [
    "# Print Perplexity:\n",
    "print('\\nPerplexity:', lda_model.log_perplexity(corpus_bow))\n",
    "\n",
    "# Compute for coherence score\n",
    "coherence_score_lda = CoherenceModel(model=lda_model, texts=processed_words, dictionary=dictionary, coherence='c_v')\n",
    "coherence_score = coherence_score_lda.get_coherence()\n",
    "# Print Coherence score:\n",
    "print('\\nCoherence Score:', coherence_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA using TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity: -6.525632269902833\n",
      "\n",
      "Coherence Score: 0.3643418335681687\n"
     ]
    }
   ],
   "source": [
    "# Print Perplexity:\n",
    "print('\\nPerplexity:', lda_model_tfidf.log_perplexity(corpus_tfidf))\n",
    "\n",
    "# Compute for coherence score\n",
    "coherence_score_lda_tf = CoherenceModel(model=lda_model_tfidf, texts=processed_words, dictionary=dictionary, coherence='c_v')\n",
    "coherence_score_tf = coherence_score_lda_tf.get_coherence()\n",
    "# Print Coherence Score:\n",
    "print('\\nCoherence Score:', coherence_score_tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform topics into Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_listScore(topic):\n",
    "    '''\n",
    "    Get scores and put it in a list\n",
    "    e.g. [1,2,3,4]\n",
    "    \n",
    "    Note: There are corpus that does not have a topic,\n",
    "    so it gets an error of IndexError, to combat the IndexError,\n",
    "    append 0 instead.\n",
    "    '''\n",
    "    scores = []\n",
    "    for i in range(len(corpus_bow)):\n",
    "        try:\n",
    "            score = lda_model_tfidf[corpus_bow[i]][topic][1]\n",
    "            scores.append(score)\n",
    "        except IndexError:\n",
    "            scores.append(0)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty dictionary_scores\n",
    "dictionary_scores = {}\n",
    "\n",
    "# Set dictionary\n",
    "numTopics = 10\n",
    "\n",
    "for topic in range(numTopics):\n",
    "    key = 'Topic ' + str(topic)\n",
    "    dictionary_scores[key] = get_listScore(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic 0</th>\n",
       "      <th>Topic 1</th>\n",
       "      <th>Topic 2</th>\n",
       "      <th>Topic 3</th>\n",
       "      <th>Topic 4</th>\n",
       "      <th>Topic 5</th>\n",
       "      <th>Topic 6</th>\n",
       "      <th>Topic 7</th>\n",
       "      <th>Topic 8</th>\n",
       "      <th>Topic 9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.849985</td>\n",
       "      <td>0.016668</td>\n",
       "      <td>0.016669</td>\n",
       "      <td>0.016668</td>\n",
       "      <td>0.016668</td>\n",
       "      <td>0.016668</td>\n",
       "      <td>0.016668</td>\n",
       "      <td>0.016672</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.016667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.033335</td>\n",
       "      <td>0.699976</td>\n",
       "      <td>0.033337</td>\n",
       "      <td>0.033337</td>\n",
       "      <td>0.033338</td>\n",
       "      <td>0.033337</td>\n",
       "      <td>0.033335</td>\n",
       "      <td>0.033336</td>\n",
       "      <td>0.033334</td>\n",
       "      <td>0.033334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.025002</td>\n",
       "      <td>0.025002</td>\n",
       "      <td>0.025012</td>\n",
       "      <td>0.774960</td>\n",
       "      <td>0.025004</td>\n",
       "      <td>0.025007</td>\n",
       "      <td>0.025003</td>\n",
       "      <td>0.025003</td>\n",
       "      <td>0.025002</td>\n",
       "      <td>0.025006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.025003</td>\n",
       "      <td>0.025015</td>\n",
       "      <td>0.025002</td>\n",
       "      <td>0.025014</td>\n",
       "      <td>0.025006</td>\n",
       "      <td>0.774933</td>\n",
       "      <td>0.025009</td>\n",
       "      <td>0.025002</td>\n",
       "      <td>0.025003</td>\n",
       "      <td>0.337901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.012502</td>\n",
       "      <td>0.012504</td>\n",
       "      <td>0.012504</td>\n",
       "      <td>0.545986</td>\n",
       "      <td>0.354294</td>\n",
       "      <td>0.012505</td>\n",
       "      <td>0.012502</td>\n",
       "      <td>0.012501</td>\n",
       "      <td>0.012502</td>\n",
       "      <td>0.012501</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Topic 0   Topic 1   Topic 2   Topic 3   Topic 4   Topic 5   Topic 6  \\\n",
       "0  0.849985  0.016668  0.016669  0.016668  0.016668  0.016668  0.016668   \n",
       "1  0.033335  0.699976  0.033337  0.033337  0.033338  0.033337  0.033335   \n",
       "2  0.025002  0.025002  0.025012  0.774960  0.025004  0.025007  0.025003   \n",
       "3  0.025003  0.025015  0.025002  0.025014  0.025006  0.774933  0.025009   \n",
       "4  0.012502  0.012504  0.012504  0.545986  0.354294  0.012505  0.012502   \n",
       "\n",
       "    Topic 7   Topic 8   Topic 9  \n",
       "0  0.016672  0.016667  0.016667  \n",
       "1  0.033336  0.033334  0.033334  \n",
       "2  0.025003  0.025002  0.025006  \n",
       "3  0.025002  0.025003  0.337901  \n",
       "4  0.012501  0.012502  0.012501  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert dictionary to dataFrame\n",
    "topic_features = pd.DataFrame(dictionary_scores)\n",
    "topic_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save as topic_features.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_features.to_csv('C:/Users/cherryb/Desktop/Personal Projects/Datasets/Telus - Fintech/results/topic_features.tsv', sep='\\t')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
