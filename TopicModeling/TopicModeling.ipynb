{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling Using LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description: Topic Modeling is a type of statistical modeling for discovering the abstract topics that occur in a collection of documents.\n",
    "\n",
    "Latent Dirichlet Allocation (LDA) is an example of topic model and is used to classify text in a document to a particular topic. It builds a topic per document model and words per topic model, modeled as Dirichlet distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora, models\n",
    "from pprint import pprint\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>by</th>\n",
       "      <th>category</th>\n",
       "      <th>comment_likes_count</th>\n",
       "      <th>comments_base</th>\n",
       "      <th>comments_count_fb</th>\n",
       "      <th>comments_replies</th>\n",
       "      <th>comments_retrieved</th>\n",
       "      <th>engagement_fb</th>\n",
       "      <th>likes_count_fb</th>\n",
       "      <th>post_id</th>\n",
       "      <th>...</th>\n",
       "      <th>post_published</th>\n",
       "      <th>rea_ANGRY</th>\n",
       "      <th>rea_HAHA</th>\n",
       "      <th>rea_LOVE</th>\n",
       "      <th>rea_SAD</th>\n",
       "      <th>rea_THANKFUL</th>\n",
       "      <th>rea_WOW</th>\n",
       "      <th>reactions_count_fb</th>\n",
       "      <th>shares_count_fb</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>post_page_155027942462</td>\n",
       "      <td>App Update</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>11</td>\n",
       "      <td>155027942462_10157348020627463</td>\n",
       "      <td>...</td>\n",
       "      <td>2019-06-27T14:30:13+0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>photo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>post_page_155027942462</td>\n",
       "      <td>Engagement</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>110</td>\n",
       "      <td>70</td>\n",
       "      <td>155027942462_10157333387457463</td>\n",
       "      <td>...</td>\n",
       "      <td>2019-06-21T14:31:06+0000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>76</td>\n",
       "      <td>4</td>\n",
       "      <td>photo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>post_page_155027942462</td>\n",
       "      <td>Engagement</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>5</td>\n",
       "      <td>155027942462_10157330985232463</td>\n",
       "      <td>...</td>\n",
       "      <td>2019-06-20T15:00:28+0000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>photo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>post_page_155027942462</td>\n",
       "      <td>Engagement</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>8</td>\n",
       "      <td>155027942462_10157323881577463</td>\n",
       "      <td>...</td>\n",
       "      <td>2019-06-17T14:30:11+0000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>photo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>post_page_155027942462</td>\n",
       "      <td>Engagement</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>6</td>\n",
       "      <td>155027942462_10157315990422463</td>\n",
       "      <td>...</td>\n",
       "      <td>2019-06-14T14:30:02+0000</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>video</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       by    category  comment_likes_count  comments_base  \\\n",
       "0  post_page_155027942462  App Update                    0              0   \n",
       "1  post_page_155027942462  Engagement                    0              0   \n",
       "2  post_page_155027942462  Engagement                    0              0   \n",
       "3  post_page_155027942462  Engagement                    0              0   \n",
       "4  post_page_155027942462  Engagement                    0              0   \n",
       "\n",
       "   comments_count_fb  comments_replies  comments_retrieved  engagement_fb  \\\n",
       "0                 11                 0                   0             22   \n",
       "1                 30                 0                   0            110   \n",
       "2                 11                 0                   0             17   \n",
       "3                 10                 0                   0             19   \n",
       "4                  8                 0                   0             17   \n",
       "\n",
       "   likes_count_fb                         post_id  ...    \\\n",
       "0              11  155027942462_10157348020627463  ...     \n",
       "1              70  155027942462_10157333387457463  ...     \n",
       "2               5  155027942462_10157330985232463  ...     \n",
       "3               8  155027942462_10157323881577463  ...     \n",
       "4               6  155027942462_10157315990422463  ...     \n",
       "\n",
       "             post_published rea_ANGRY  rea_HAHA  rea_LOVE  rea_SAD  \\\n",
       "0  2019-06-27T14:30:13+0000         0         0         0        0   \n",
       "1  2019-06-21T14:31:06+0000         1         0         4        0   \n",
       "2  2019-06-20T15:00:28+0000         1         0         0        0   \n",
       "3  2019-06-17T14:30:11+0000         1         0         0        0   \n",
       "4  2019-06-14T14:30:02+0000         3         0         0        0   \n",
       "\n",
       "   rea_THANKFUL  rea_WOW  reactions_count_fb  shares_count_fb   type  \n",
       "0             0        0                  11                0  photo  \n",
       "1             0        1                  76                4  photo  \n",
       "2             0        0                   6                0  photo  \n",
       "3             0        0                   9                0  photo  \n",
       "4             0        0                   9                0  video  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv('C:/Users/cherryb/Desktop/Personal Projects/Datasets/Telus - Fintech/cleaned/fullstatCleaned_withLabels.tsv', index_col='Unnamed: 0', sep='\\t')\n",
    "# Inspect df\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize post_message column\n",
    "df['post_message'] = df['post_message'].apply(lambda list_words: word_tokenize(list_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select df['post_message']\n",
    "processed_words = df['post_message']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of processed_words\n",
    "processed_words = [words for words in processed_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Word (BoW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary from ‘processed_words’ containing the number of times a word appears in the training set\n",
    "dictionary = corpora.Dictionary(processed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out tokens\n",
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary reporting how many words and how many times those words appear\n",
    "corpus_bow = [dictionary.doc2bow(doc) for doc in processed_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 57 (\"paydai\") appears 1 time.\n",
      "Word 68 (\"see\") appears 1 time.\n",
      "Word 108 (\"week\") appears 1 time.\n",
      "Word 118 (\"someon\") appears 1 time.\n",
      "Word 119 (\"bonu\") appears 1 time.\n",
      "Word 163 (\"winner\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "# Preview Bag of Words for a sample preprocessed document\n",
    "bow_doc_500 = corpus_bow[500]\n",
    "\n",
    "for i in range(len(bow_doc_500)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_500[i][0], dictionary[bow_doc_500[i][0]], bow_doc_500[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model\n",
    "tfidf = models.TfidfModel(corpus_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply transformation to the entire corpus\n",
    "corpus_tfidf = tfidf[corpus_bow]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.8057762139865111),\n",
      " (1, 0.3489304433755792),\n",
      " (2, 0.39392744378681527),\n",
      " (3, 0.2716494205605292)]\n"
     ]
    }
   ],
   "source": [
    "# Preview TF-IDF scores for the first document\n",
    "for doc in corpus_tfidf:\n",
    "    pprint(doc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling - LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the lda model using gensim.models.LdaMulticore and corpus_bow\n",
    "lda_model = gensim.models.LdaMulticore(corpus_bow, num_topics=10, id2word=dictionary, passes=2, workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.059*\"monei\" + 0.040*\"save\" + 0.034*\"green\" + 0.032*\"dot\" + 0.030*\"deposit\" + 0.029*\"learn\" + 0.028*\"get\" + 0.028*\"direct\" + 0.026*\"win\" + 0.022*\"time\"\n",
      "\n",
      "Topic: 1 \n",
      "Words: 0.083*\"dot\" + 0.083*\"green\" + 0.040*\"like\" + 0.030*\"page\" + 0.028*\"app\" + 0.027*\"win\" + 0.027*\"mobil\" + 0.026*\"sweepstak\" + 0.024*\"see\" + 0.024*\"u\"\n",
      "\n",
      "Topic: 2 \n",
      "Words: 0.081*\"card\" + 0.072*\"cash\" + 0.066*\"back\" + 0.046*\"green\" + 0.045*\"dot\" + 0.041*\"visa\" + 0.040*\"debit\" + 0.037*\"fee\" + 0.034*\"appli\" + 0.029*\"annual\"\n",
      "\n",
      "Topic: 3 \n",
      "Words: 0.047*\"card\" + 0.039*\"green\" + 0.038*\"dot\" + 0.027*\"like\" + 0.027*\"us\" + 0.026*\"fee\" + 0.022*\"wai\" + 0.021*\"appli\" + 0.020*\"back\" + 0.020*\"help\"\n",
      "\n",
      "Topic: 4 \n",
      "Words: 0.064*\"see\" + 0.062*\"credit\" + 0.060*\"paydai\" + 0.059*\"bonu\" + 0.056*\"week\" + 0.046*\"winner\" + 0.042*\"card\" + 0.035*\"secur\" + 0.035*\"schedul\" + 0.034*\"dot\"\n",
      "\n",
      "Topic: 5 \n",
      "Words: 0.083*\"get\" + 0.066*\"pai\" + 0.037*\"deposit\" + 0.036*\"period\" + 0.024*\"mai\" + 0.023*\"dai\" + 0.023*\"card\" + 0.023*\"time\" + 0.020*\"direct\" + 0.020*\"learn\"\n",
      "\n",
      "Topic: 6 \n",
      "Words: 0.044*\"back\" + 0.040*\"new\" + 0.039*\"tell\" + 0.036*\"u\" + 0.036*\"financi\" + 0.034*\"comment\" + 0.033*\"dai\" + 0.032*\"get\" + 0.030*\"cash\" + 0.023*\"budget\"\n",
      "\n",
      "Topic: 7 \n",
      "Words: 0.049*\"sweepstak\" + 0.035*\"win\" + 0.032*\"tax\" + 0.030*\"rule\" + 0.028*\"tip\" + 0.027*\"comment\" + 0.027*\"see\" + 0.026*\"chanc\" + 0.023*\"like\" + 0.021*\"page\"\n",
      "\n",
      "Topic: 8 \n",
      "Words: 0.050*\"u\" + 0.049*\"comment\" + 0.041*\"share\" + 0.038*\"tell\" + 0.033*\"financ\" + 0.031*\"love\" + 0.030*\"tip\" + 0.025*\"person\" + 0.022*\"summer\" + 0.019*\"entrant\"\n",
      "\n",
      "Topic: 9 \n",
      "Words: 0.064*\"read\" + 0.052*\"financi\" + 0.040*\"u\" + 0.037*\"famili\" + 0.029*\"month\" + 0.029*\"learn\" + 0.029*\"good\" + 0.029*\"know\" + 0.027*\"mondaymotiv\" + 0.027*\"see\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For each topic, explore the words occuring in that topic and its relative weight.\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}\\n'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the lda model using gensim.models.LdaMulticore and corpus_tfidf\n",
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=10, id2word=dictionary, passes=2, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Word: 0.052*\"wai\" + 0.050*\"save\" + 0.029*\"love\" + 0.029*\"monei\" + 0.024*\"tip\" + 0.023*\"plan\" + 0.023*\"on\" + 0.022*\"summer\" + 0.019*\"smart\" + 0.018*\"stai\"\n",
      "\n",
      "Topic: 1 \n",
      "Word: 0.034*\"person\" + 0.032*\"someon\" + 0.031*\"ontheblog\" + 0.030*\"mobil\" + 0.029*\"app\" + 0.028*\"monei\" + 0.023*\"see\" + 0.019*\"balanc\" + 0.018*\"dai\" + 0.017*\"mondaymotiv\"\n",
      "\n",
      "Topic: 2 \n",
      "Word: 0.042*\"read\" + 0.033*\"schedul\" + 0.030*\"ssi\" + 0.029*\"pai\" + 0.029*\"season\" + 0.028*\"payment\" + 0.026*\"get\" + 0.023*\"bill\" + 0.022*\"favorit\" + 0.019*\"secur\"\n",
      "\n",
      "Topic: 3 \n",
      "Word: 0.047*\"cash\" + 0.044*\"back\" + 0.039*\"card\" + 0.031*\"debit\" + 0.030*\"fee\" + 0.030*\"visa\" + 0.027*\"appli\" + 0.024*\"bank\" + 0.024*\"annual\" + 0.024*\"dot\"\n",
      "\n",
      "Topic: 4 \n",
      "Word: 0.033*\"budget\" + 0.028*\"good\" + 0.025*\"dai\" + 0.025*\"thank\" + 0.023*\"list\" + 0.022*\"paid\" + 0.022*\"read\" + 0.021*\"winner\" + 0.020*\"see\" + 0.019*\"get\"\n",
      "\n",
      "Topic: 5 \n",
      "Word: 0.035*\"famili\" + 0.026*\"back\" + 0.025*\"make\" + 0.024*\"card\" + 0.024*\"cash\" + 0.022*\"fun\" + 0.022*\"credit\" + 0.021*\"start\" + 0.019*\"talk\" + 0.018*\"learn\"\n",
      "\n",
      "Topic: 6 \n",
      "Word: 0.042*\"tip\" + 0.033*\"comment\" + 0.031*\"monei\" + 0.031*\"like\" + 0.027*\"page\" + 0.026*\"u\" + 0.023*\"financi\" + 0.023*\"treat\" + 0.019*\"budgetfriendli\" + 0.019*\"save\"\n",
      "\n",
      "Topic: 7 \n",
      "Word: 0.046*\"week\" + 0.042*\"financi\" + 0.040*\"bonu\" + 0.040*\"winner\" + 0.038*\"paydai\" + 0.033*\"see\" + 0.030*\"check\" + 0.027*\"learn\" + 0.026*\"free\" + 0.020*\"win\"\n",
      "\n",
      "Topic: 8 \n",
      "Word: 0.040*\"year\" + 0.030*\"financ\" + 0.028*\"new\" + 0.027*\"win\" + 0.027*\"look\" + 0.026*\"u\" + 0.023*\"prize\" + 0.019*\"deposit\" + 0.018*\"sweepstak\" + 0.018*\"chanc\"\n",
      "\n",
      "Topic: 9 \n",
      "Word: 0.051*\"dot\" + 0.050*\"green\" + 0.029*\"share\" + 0.027*\"comment\" + 0.026*\"entrant\" + 0.022*\"budget\" + 0.020*\"credit\" + 0.019*\"extra\" + 0.019*\"happi\" + 0.019*\"u\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For each topic, explore the words occuring in that topic and its relative weight.\n",
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} \\nWord: {}\\n'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification of the Topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance evaluation by classifying sample document using LDA Bag of Words model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['someon', 'cloud', 'nine', 'see', 'week', 'paydai', 'bonu', 'winner']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_words[500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.871422529220581\t \n",
      "Topic: 0.064*\"see\" + 0.062*\"credit\" + 0.060*\"paydai\" + 0.059*\"bonu\" + 0.056*\"week\" + 0.046*\"winner\" + 0.042*\"card\" + 0.035*\"secur\" + 0.035*\"schedul\" + 0.034*\"dot\"\n",
      "\n",
      "Score: 0.014287102967500687\t \n",
      "Topic: 0.050*\"u\" + 0.049*\"comment\" + 0.041*\"share\" + 0.038*\"tell\" + 0.033*\"financ\" + 0.031*\"love\" + 0.030*\"tip\" + 0.025*\"person\" + 0.022*\"summer\" + 0.019*\"entrant\"\n",
      "\n",
      "Score: 0.014286868274211884\t \n",
      "Topic: 0.044*\"back\" + 0.040*\"new\" + 0.039*\"tell\" + 0.036*\"u\" + 0.036*\"financi\" + 0.034*\"comment\" + 0.033*\"dai\" + 0.032*\"get\" + 0.030*\"cash\" + 0.023*\"budget\"\n",
      "\n",
      "Score: 0.014286590740084648\t \n",
      "Topic: 0.064*\"read\" + 0.052*\"financi\" + 0.040*\"u\" + 0.037*\"famili\" + 0.029*\"month\" + 0.029*\"learn\" + 0.029*\"good\" + 0.029*\"know\" + 0.027*\"mondaymotiv\" + 0.027*\"see\"\n",
      "\n",
      "Score: 0.014286577701568604\t \n",
      "Topic: 0.083*\"dot\" + 0.083*\"green\" + 0.040*\"like\" + 0.030*\"page\" + 0.028*\"app\" + 0.027*\"win\" + 0.027*\"mobil\" + 0.026*\"sweepstak\" + 0.024*\"see\" + 0.024*\"u\"\n",
      "\n",
      "Score: 0.014286219142377377\t \n",
      "Topic: 0.049*\"sweepstak\" + 0.035*\"win\" + 0.032*\"tax\" + 0.030*\"rule\" + 0.028*\"tip\" + 0.027*\"comment\" + 0.027*\"see\" + 0.026*\"chanc\" + 0.023*\"like\" + 0.021*\"page\"\n",
      "\n",
      "Score: 0.014286156743764877\t \n",
      "Topic: 0.059*\"monei\" + 0.040*\"save\" + 0.034*\"green\" + 0.032*\"dot\" + 0.030*\"deposit\" + 0.029*\"learn\" + 0.028*\"get\" + 0.028*\"direct\" + 0.026*\"win\" + 0.022*\"time\"\n",
      "\n",
      "Score: 0.014286086894571781\t \n",
      "Topic: 0.047*\"card\" + 0.039*\"green\" + 0.038*\"dot\" + 0.027*\"like\" + 0.027*\"us\" + 0.026*\"fee\" + 0.022*\"wai\" + 0.021*\"appli\" + 0.020*\"back\" + 0.020*\"help\"\n",
      "\n",
      "Score: 0.0142859797924757\t \n",
      "Topic: 0.083*\"get\" + 0.066*\"pai\" + 0.037*\"deposit\" + 0.036*\"period\" + 0.024*\"mai\" + 0.023*\"dai\" + 0.023*\"card\" + 0.023*\"time\" + 0.020*\"direct\" + 0.020*\"learn\"\n",
      "\n",
      "Score: 0.014285930432379246\t \n",
      "Topic: 0.081*\"card\" + 0.072*\"cash\" + 0.066*\"back\" + 0.046*\"green\" + 0.045*\"dot\" + 0.041*\"visa\" + 0.040*\"debit\" + 0.037*\"fee\" + 0.034*\"appli\" + 0.029*\"annual\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model[corpus_bow[500]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance evaluation by classifying sample document using LDA TF-IDF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.8714191317558289\t \n",
      "Topic: 0.046*\"week\" + 0.042*\"financi\" + 0.040*\"bonu\" + 0.040*\"winner\" + 0.038*\"paydai\" + 0.033*\"see\" + 0.030*\"check\" + 0.027*\"learn\" + 0.026*\"free\" + 0.020*\"win\"\n",
      "\n",
      "Score: 0.014289558865129948\t \n",
      "Topic: 0.034*\"person\" + 0.032*\"someon\" + 0.031*\"ontheblog\" + 0.030*\"mobil\" + 0.029*\"app\" + 0.028*\"monei\" + 0.023*\"see\" + 0.019*\"balanc\" + 0.018*\"dai\" + 0.017*\"mondaymotiv\"\n",
      "\n",
      "Score: 0.014286858029663563\t \n",
      "Topic: 0.051*\"dot\" + 0.050*\"green\" + 0.029*\"share\" + 0.027*\"comment\" + 0.026*\"entrant\" + 0.022*\"budget\" + 0.020*\"credit\" + 0.019*\"extra\" + 0.019*\"happi\" + 0.019*\"u\"\n",
      "\n",
      "Score: 0.014286745339632034\t \n",
      "Topic: 0.033*\"budget\" + 0.028*\"good\" + 0.025*\"dai\" + 0.025*\"thank\" + 0.023*\"list\" + 0.022*\"paid\" + 0.022*\"read\" + 0.021*\"winner\" + 0.020*\"see\" + 0.019*\"get\"\n",
      "\n",
      "Score: 0.014286553487181664\t \n",
      "Topic: 0.047*\"cash\" + 0.044*\"back\" + 0.039*\"card\" + 0.031*\"debit\" + 0.030*\"fee\" + 0.030*\"visa\" + 0.027*\"appli\" + 0.024*\"bank\" + 0.024*\"annual\" + 0.024*\"dot\"\n",
      "\n",
      "Score: 0.014286398887634277\t \n",
      "Topic: 0.035*\"famili\" + 0.026*\"back\" + 0.025*\"make\" + 0.024*\"card\" + 0.024*\"cash\" + 0.022*\"fun\" + 0.022*\"credit\" + 0.021*\"start\" + 0.019*\"talk\" + 0.018*\"learn\"\n",
      "\n",
      "Score: 0.014286291785538197\t \n",
      "Topic: 0.040*\"year\" + 0.030*\"financ\" + 0.028*\"new\" + 0.027*\"win\" + 0.027*\"look\" + 0.026*\"u\" + 0.023*\"prize\" + 0.019*\"deposit\" + 0.018*\"sweepstak\" + 0.018*\"chanc\"\n",
      "\n",
      "Score: 0.01428623590618372\t \n",
      "Topic: 0.052*\"wai\" + 0.050*\"save\" + 0.029*\"love\" + 0.029*\"monei\" + 0.024*\"tip\" + 0.023*\"plan\" + 0.023*\"on\" + 0.022*\"summer\" + 0.019*\"smart\" + 0.018*\"stai\"\n",
      "\n",
      "Score: 0.014286180026829243\t \n",
      "Topic: 0.042*\"read\" + 0.033*\"schedul\" + 0.030*\"ssi\" + 0.029*\"pai\" + 0.029*\"season\" + 0.028*\"payment\" + 0.026*\"get\" + 0.023*\"bill\" + 0.022*\"favorit\" + 0.019*\"secur\"\n",
      "\n",
      "Score: 0.014286091551184654\t \n",
      "Topic: 0.042*\"tip\" + 0.033*\"comment\" + 0.031*\"monei\" + 0.031*\"like\" + 0.027*\"page\" + 0.026*\"u\" + 0.023*\"financi\" + 0.023*\"treat\" + 0.019*\"budgetfriendli\" + 0.019*\"save\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model_tfidf[corpus_bow[500]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model_tfidf.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform topics into Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_listScore(topic):\n",
    "    '''\n",
    "    Get scores and put it in a list\n",
    "    e.g. [1,2,3,4]\n",
    "    \n",
    "    Note: There are corpus that does not have a topic,\n",
    "    so it gets an error of IndexError, to combat the IndexError,\n",
    "    append 0 instead.\n",
    "    '''\n",
    "    scores = []\n",
    "    for i in range(len(corpus_bow)):\n",
    "        try:\n",
    "            score = lda_model_tfidf[corpus_bow[i]][topic][1]\n",
    "            scores.append(score)\n",
    "        except IndexError:\n",
    "            scores.append(0)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty dictionary_scores\n",
    "dictionary_scores = {}\n",
    "\n",
    "# Set dictionary\n",
    "numTopics = 10\n",
    "\n",
    "for topic in range(numTopics):\n",
    "    key = 'Topic ' + str(topic)\n",
    "    dictionary_scores[key] = get_listScore(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic 0</th>\n",
       "      <th>Topic 1</th>\n",
       "      <th>Topic 2</th>\n",
       "      <th>Topic 3</th>\n",
       "      <th>Topic 4</th>\n",
       "      <th>Topic 5</th>\n",
       "      <th>Topic 6</th>\n",
       "      <th>Topic 7</th>\n",
       "      <th>Topic 8</th>\n",
       "      <th>Topic 9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.601591</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.265269</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.016668</td>\n",
       "      <td>0.016668</td>\n",
       "      <td>0.016668</td>\n",
       "      <td>0.016668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.699907</td>\n",
       "      <td>0.033349</td>\n",
       "      <td>0.033340</td>\n",
       "      <td>0.033338</td>\n",
       "      <td>0.033356</td>\n",
       "      <td>0.033345</td>\n",
       "      <td>0.033342</td>\n",
       "      <td>0.033342</td>\n",
       "      <td>0.033339</td>\n",
       "      <td>0.033343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.774977</td>\n",
       "      <td>0.025003</td>\n",
       "      <td>0.025002</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.025001</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.025009</td>\n",
       "      <td>0.025003</td>\n",
       "      <td>0.025001</td>\n",
       "      <td>0.025003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.025010</td>\n",
       "      <td>0.025006</td>\n",
       "      <td>0.025004</td>\n",
       "      <td>0.025001</td>\n",
       "      <td>0.025005</td>\n",
       "      <td>0.025007</td>\n",
       "      <td>0.774959</td>\n",
       "      <td>0.025002</td>\n",
       "      <td>0.025002</td>\n",
       "      <td>0.025005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.887463</td>\n",
       "      <td>0.012505</td>\n",
       "      <td>0.012502</td>\n",
       "      <td>0.012511</td>\n",
       "      <td>0.012505</td>\n",
       "      <td>0.012502</td>\n",
       "      <td>0.012502</td>\n",
       "      <td>0.012502</td>\n",
       "      <td>0.012503</td>\n",
       "      <td>0.012504</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Topic 0   Topic 1   Topic 2   Topic 3   Topic 4   Topic 5   Topic 6  \\\n",
       "0  0.016667  0.016667  0.601591  0.016667  0.265269  0.016667  0.016668   \n",
       "1  0.699907  0.033349  0.033340  0.033338  0.033356  0.033345  0.033342   \n",
       "2  0.774977  0.025003  0.025002  0.025000  0.025001  0.025000  0.025009   \n",
       "3  0.025010  0.025006  0.025004  0.025001  0.025005  0.025007  0.774959   \n",
       "4  0.887463  0.012505  0.012502  0.012511  0.012505  0.012502  0.012502   \n",
       "\n",
       "    Topic 7   Topic 8   Topic 9  \n",
       "0  0.016668  0.016668  0.016668  \n",
       "1  0.033342  0.033339  0.033343  \n",
       "2  0.025003  0.025001  0.025003  \n",
       "3  0.025002  0.025002  0.025005  \n",
       "4  0.012502  0.012503  0.012504  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert dictionary to dataFrame\n",
    "topic_features = pd.DataFrame(dictionary_scores)\n",
    "topic_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save as topic_features.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_features.to_csv('C:/Users/cherryb/Desktop/Personal Projects/Datasets/Telus - Fintech/results/topic_features.tsv', sep='\\t')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
